---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

# 👨‍💻 About Me
I am now a third-year PhD student in Beijing Jiaotong University, advised by Prof. Yufeng Chen and Prof. Jinan Xu. Before that, I started my master's degree in 2020 and transferred to the doctoral program in 2022. I am now interning at WeChat, Tencent, working on LLM safety.

My research interest is mainly on training algorithms for LLMs, especially the ones that enhance the model capacity with limited model size, like knowledge distillation and RL techniques. Now I am also interested in LLM safety, seeking the balance between model safety and model intelligence.


# 🔥 News
- *2025.05*: &nbsp;🎉🎉 Two papers have been accepted by ACL2025! See you in Vienna. 
- *2025.03*: [DSKDv2](https://arxiv.org/abs/2504.11426) has been released on arxiv. 
- *2024.09*: &nbsp;🎉🎉 [DSKD](https://arxiv.org/abs/2406.17328) has been accepted by EMNLP2024! 

# 📝 Selected Papers 

<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2016</div><img src='images/500x300.png' alt="sym" width="100%"></div></div> -->
<!-- <div class='paper-box-text' markdown="1"> -->
## Preprint

- [A Dual-Space Framework for General Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2504.11426)

  Xue Zhang*, **Songming Zhang\***, Yunlong Liang, Fandong Meng, Yufeng Chen, Jinan Xu, Jie Zhou

  ---

## Publications

- [AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation](https://arxiv.org/abs/2503.02832) (**ACL 2025 Main**)

  **Songming Zhang**, Xue Zhang, Tong Zhang, Bojie Hu, Yufeng Chen, Jinan Xu

- [Dual-Space Knowledge Distillation for Large Language Models](https://arxiv.org/abs/2406.17328) (**EMNLP 2024 Main**)

  **Songming Zhang**, Xue Zhang, Zengkui Sun, Yufeng Chen, Jinan Xu

- [Towards Understanding and Improving Knowledge Distillation
for Neural Machine Translation](https://arxiv.org/abs/2305.08096) (**ACL 2023 Main**)

  **Songming Zhang**, Yunlong Liang, Shuaibo Wang, Yufeng Chen, Wenjuan Han, Jian Liu, Jinan Xu

- [Conditional Bilingual Mutual Information Based Adaptive Training for Neural Machine Translation](https://arxiv.org/abs/2203.02951) (**ACL 2022 Main**)

  **Songming Zhang\***, Yijin Liu*, Fandong Meng, Yufeng Chen, Jinan Xu, Jian Liu, Jie Zhou

  [[Full paper list](https://scholar.google.com/citations?user=u_bYOuYAAAAJ&hl=zh-CN)]

<!-- [**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div> -->

<!-- - [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020** -->

<!-- # 🎖 Honors and Awards
- *2021.10* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.09* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->

# 📖 Educations
- *2022.09 - Now*, Beijing Jiaotong University, PhD Student. 
- *2020.09 - 2022.06*, Beijing Jiaotong University, Master.
- *2016.09 - 2020.06*, Beijing University of Chemical Technology, Bachelor. 

<!-- # 💬 Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/) -->

# 💻 Internships
- *2024.07 - Now*, WeChat, Tencent.
- *2022.02 - 2024.03*, TI Cloud Inc.
- *2021.01 - 2022.01*, WeChat AI, Tencent.

# 🔖 Services
- ACL 2023, Reviewer
- ACL ARR 2023-2025, Reviewer